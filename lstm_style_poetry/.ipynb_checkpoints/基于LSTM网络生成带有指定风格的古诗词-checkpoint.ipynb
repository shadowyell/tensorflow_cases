{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 基于LSTM网络生成带有指定风格的古诗词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 一、\t**项目环境**\n",
    "1.\tpython 3.6\n",
    "2.\ttensorflow 1.2\n",
    "3.\tmac环境\n",
    "4.\t诗词预料包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、\t项目简介\n",
    "通过构建LSTM神经网络，训练神经网络，建立一个语言模型，学习古诗词的语言特征。同时让模型学习到不同风格的古诗词，并且提取相应的特质，结合到新的古诗词中。整个项目分为4个模块，如下:\n",
    "\n",
    "LSTM_model.py：LSTM网络模型，提供了end_points接口，被其他部分调用\n",
    "\n",
    "poetry_porcess.py：数据读取、预处理部分，会返回打包好的batch，被main调用\n",
    "\n",
    "gen_poetry.py：古诗生成程序，拥有可选的风格参数，被main调用\n",
    "\n",
    "main.py：主函数，既可以调用前两个程序获取预处理数据并使用LSTM网络进行训练，也可以调用gen_poetry.py生成古诗\n",
    "\n",
    "在`main.py`最后有如下指令，\n",
    "```Python\n",
    "if __name__ == \"__main__\":\n",
    "    words,poetry_vector,to_num,x_batches,y_batches = poetry_porcess.poetry_process()\n",
    "    # train(words, poetry_vector, x_batches, y_batches)\n",
    "    # gen_poetry(words, to_num)\n",
    "    generate(words_, to_num_, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\")\n",
    "```\n",
    "此时实际上处于生成模式，对于最后的三行:\n",
    "\n",
    "train：表示训练\n",
    "\n",
    "gen_poetry：表示根据首字符生成\n",
    "\n",
    "generate：表示根据首句和风格句生成古诗\n",
    "\n",
    "需要实现哪个功能就将其它功能注释掉。接下来让我们具体的讨论每一个模块的功能与实现。\n",
    "\n",
    "由于方便在线演示，在文章的最后，我们将所有功能整合为一个python文件，进行运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、文本预处理\n",
    "  本节来简单介绍一下中文文字的预处理流程，其实有一大部分深度学习中，对于中文文本的处理就是接下来要介绍的方法，所以该方法具有普遍的参考意义，但也有部分项目通过word2vec来进行文本的预处理，本文介绍的是通过文字与ID的映射，再通过embedding来训练文本。整个部分分为两个部分：第一部分就文本的处理，主要的任务就是读取预料，进行一些简单的去杂操作。第二部分就是对文档进行文字与ID的映射，即辅助数据结构生成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一部分：文本处理，重点在于读取+去除特殊符号\n",
    "\n",
    "核心代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(poetry_file,'r',encoding='utf-8') as f:\n",
    "        # 按行读取古诗词\n",
    "        for line in f:\n",
    "            try:\n",
    "                # 将古诗词的诗名和内容分开\n",
    "                title,content = line.strip().split(':')\n",
    "                # 去空格，实际上没用到，但为了确保文档中没有多余的空格，还是建议写一下\n",
    "                content = content.replace(' ','') \n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content:\n",
    "                    continue\n",
    "                # 选取长度在5～79之间的古诗词\n",
    "                if len(content) < 5 or len(content) > 79:\n",
    "                    continue\n",
    "                # 添加首尾标签\n",
    "                content = 'B' + content + 'E'\n",
    "                # 添加到poetrys列表中\n",
    "                poetrys.append(content)\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二部分：辅助数据结构生成\n",
    "\n",
    "1.使用counter = collections.Counter()函数对所有字符进行计数。\n",
    "\n",
    "核心代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "    for poetry in poetrys:\n",
    "        all_words += [word for word in poetry]\n",
    "    counter = Counter(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.通过count_pairs = sorted(counter.items(), key=lambda x: -x[1])对计数结果进行排序，返回的结果是一个tuple\n",
    "\n",
    "3.通过words, _ = zip(*count_pairs)对tuple进行解压，得到words列表代表所有字符。该字符的行号即为该字符索引\n",
    "\n",
    "4.通过word_int_map = dict(zip(words, range(len(words))))，得到字符与行号对应的索引字典\n",
    "\n",
    "5.通过poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]函数将字符数据集映射成为索引\n",
    "\n",
    "核心代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    words = words[:len(words)] + (' ',)  # 后面要用' '来补齐诗句长度\n",
    "    # print(words)\n",
    "    # 字典:word->int\n",
    "    # 每个字映射为一个数字ID  \n",
    "    word_num_map = dict(zip(words,range(len(words))))\n",
    "    # 把诗词转换为向量\n",
    "    to_num = lambda word: word_num_map.get(word,len(words))\n",
    "    #print(to_num)\n",
    "    poetry_vector = [list(map(to_num,poetry)) for poetry in poetrys]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、 预料准备\n",
    "  该深度学习是一个有监督（带标签的）的学习模型，所以我们需要准备训练数据集。由于生成古诗词的本质是一种语言模型的建立，简单来说就是给定第一个字，生成第二个字，生成第三个字，从而生成一句诗词，使得该诗词符合自然语言的表达。所以我们准备的预料也是前一个字是输入，而后一个则是我们的预测数据，这样的模型我们有一个专门的模型名称，叫做序列模型，就想流水一样，一个字接一个字。到此为止，我们已经介绍了我们模型需要的数据集，还有一个问题，就是当我们进行训练的时候，需要定义batch大小，通过并行化提高内存利用率。让我们来实现这两个要求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先来求出诗向量的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " n_chunk = len(poetry_vector) // batch_size\n",
    "  #同时定义我们的输入数据X，以及输出数据Y\n",
    "    x_batches = []\n",
    "y_batches = []\n",
    "  #通过start和end标签来实现batch大小的设置。\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size #起始位置\n",
    "        end_index = start_index + batch_size #结束位置\n",
    "        batches = poetry_vector[start_index:end_index]\n",
    "        length = max(map(len,batches))  # 记录下最长的诗句的长度\n",
    "  #补全同一批测试集数据中的句子长度大小\n",
    "        xdata = np.full((batch_size,length),word_num_map[' '],np.int32)\n",
    "  #实现前一个字与后一个字之间的生成关系\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:,:-1] = xdata[:,1:]\n",
    "        \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、模型建立\n",
    "  本节要讨论是如何用tensorflow建立一个lstm模型。\n",
    "  \n",
    "  首先需要定义一个带有LSTM的RNN网络。 \n",
    "  \n",
    "定义如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model(num_of_word,input_data,output_data=None,rnn_size=128,num_layers=2,batch_size=128):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，参数意义如下：\n",
    "param num_of_word: 词的个数\n",
    "\n",
    "param input_data: 输入向量\n",
    "\n",
    "param output_data: 标签\n",
    "\n",
    "param rnn_size: 隐藏层的向量尺寸\n",
    "\n",
    "param num_layers: 隐藏层的层数\n",
    "\n",
    "param batch_size: 批处理数据大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义LSTM的核心功能，定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_fun = tf.contrib.rnn.BasicLSTMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先使用tf.nn.rnn_cell.BasicLSTMCell定义单个基本的LSTM单元。这里的size其实就是hidden_size。 在LSTM单元中，有2个状态值，分别是c和h，分别对应于下图中的c和h。其中h在作为当前时间段的输出的同时，也是下一时间段的输入的一部分。\n",
    "<img src=\"./图片 1.png\" style=\"max-width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么当state_is_tuple=True的时候，state是元组形式，state=(c,h)。如果是False，那么state是一个由c和h拼接起来的张量，state=tf.concat(1,[c,h])。在运行时，则返回2值，一个是h，还有一个state。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " cell = cell_fun(rnn_size,state_is_tuple=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个示例中，我们使用了2层的LSTM网络。也就是说，前一层的LSTM的输出作为后一层的输入。使用tf.nn.rnn_cell.MultiRNNCell可以实现这个功能。这个基本没什么好说的，state_is_tuple用法也跟之前的类似。构造完多层LSTM以后，使用zero_state即可对各种状态进行初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell =tf.contrib.rnn.MultiRNNCell([cell] * num_layers,state_is_tuple=True)\n",
    "    # 如果有标签(output_data)则初始化一个batch的cell状态，否则初始化一个\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1,tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们进行词向量的嵌入\n",
    "\n",
    "•\t构造一个num_of_word＋1 x rnn_size的矩阵，作为embedding容器\n",
    "\n",
    "•\t有num_of_word＋1个容量为rnn_size的向量，每个向量代表一个vocabulary，每个向量的中的分量的值都在-1到1之间随机分布\n",
    "\n",
    "之前有提到过，输入模型的input和target都是用词典id表示的。例如一个句子，“我/是/学生”，这三个词在词典中的序号分别是0,5,3，那么上面的句子就是[0,5,3]。显然这个是不能直接用的，我们要把词典id转化成向量,也就是embedding形式。实现的方法很简单。\n",
    "\n",
    "•\t第一步，构建一个矩阵，就叫embedding好了，尺寸为[vocab_size, embedding_size]，分别表示词典中单词数目，以及要转化成的向量的维度。一般来说，向量维度越高，能够表现的信息也就越丰富。\n",
    "\n",
    "•\t第二步，使用tf.nn.embedding_lookup(embedding,input_ids) 假设input_ids的长度为len，那么返回的张量尺寸就为[len,embedding_size]。\n",
    "\n",
    "具体实现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding =tf.get_variable('embedding',initializer=tf.random_uniform([num_of_word + 1,rnn_size],-1.0,1.0))\n",
    "inputs = tf.nn.embedding_lookup(embedding,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成embedding工作之后，让我们来考虑一个问题，在每一个train step，传入model的是一个batch的数据（这一个batch的数据forward得到predictions，计算loss，backpropagation更新参数），这一个batch内的数据一定是padding成相同长度的。\n",
    "那么，如果可以只在一个batch内部进行padding，例如一个batch中数据长度均在6-10这个范围内，就可以让这个batch中所有数据pad到固定长度10，而整个dataset上的数据最大长度很可能是100，这样就不需要让这些数据也pad到100那么长，白白浪费空间。\n",
    "我们考虑dynamic_rnn，这个函数实现的功能就是可以让不同迭代传入的batch可以是长度不同数据，但同一次迭代一个batch内部的所有数据长度仍然是固定的。例如，第一时刻传入的数据shape=[batch_size, 10]，第二时刻传入的数据shape=[batch_size, 12]，第三时刻传入的数据shape=[batch_size, 8]等等。\n",
    "但是rnn不能这样，它要求每一时刻传入的batch数据的[batch_size, max_seq]，在每次迭代过程中都保持不变。\n",
    "所以我们来实现一下这个功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs,last_state =\n",
    "tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state)\n",
    "output = tf.reshape(outputs,[-1,rnn_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们来看一下W，b的参数设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([rnn_size,num_of_word + 1]))\n",
    "bias = tf.Variable(tf.zeros(shape=[num_of_word + 1]))\n",
    "logits = tf.nn.bias_add(tf.matmul(output,weights),bias=bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们看一下对于损失函数的定义，以及优化器的定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此为止，我们的LSTM模型就搭建完了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、训练\n",
    "在完成模型的搭建以后，我们就要开始进行模型的训练，定义训练函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(words,poetry_vector,x_batches,y_batches):\n",
    "#然后定义输入数据，以及输出标签的容器，来存放数据。\n",
    "    input_data = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    output_targets = tf.placeholder(tf.int32,[batch_size,None])\n",
    "#同时，定义数据保存节点：\n",
    "    end_points = rnn_model(len(words),input_data=input_data,output_data=output_targets,batch_size=batch_size)\n",
    "#建立svaer类，用来保存训练得到的参数\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "#进行初始化\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "#为了更直观的展示训练过程，进行可视化处理，将图形、训练过程等数据合并在一起\n",
    "    merge = tf.summary.merge_all()\n",
    "#通过上下文管理机制，建立一个新会话，来完成\n",
    "    with tf.Session(config=config) as sess:\n",
    "#将训练日志写在log文件夹下\n",
    "        writer = tf.summary.FileWriter('./logs',sess.graph)\n",
    "#开始训练,并建立模型保存目录\n",
    "        sess.run(init_op)\n",
    "        start_epoch = 0\n",
    "        model_dir = \"./model\"\n",
    "        epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练的过程中，我们很有可能在训练到一半的时候出现意外导致训练停止，所以我们要及时记录已经训练的数据，从而不用在出现意外的时候从头训练，而是接着上次训练继续，所以我们需要设置一个检查点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess,checkpoint)\n",
    "            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n",
    "            start_epoch += int(checkpoint.split('-')[-1])\n",
    "            print('## start training...')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、\t诗歌生成\n",
    "在完成训练模型以后，我们就需要利用训练完的模型进行诗歌生成。\n",
    "该部分代码主要包含gen_poem和to_word函数。就是将训练结束后的last_state作为initial_state传入lstm，那么新生成的output即为预测的结果。\n",
    "首先我们需要定义一个to_word函数，大家是否还记得，我们在处理预料数据的时候，定义过一个to_mun函数，作用是将文字转化为数字，而这里的to_word函数功能恰恰相反根据生成的词向量转换为文字。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_word(predict, vocabs):\n",
    "    t = np.cumsum(predict)\n",
    "    s = np.sum(predict)\n",
    "    sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    #t的长度为vocab_size + 1, 随机生成一个数然后判断能插入第几个位置来取字\n",
    "    sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]  # [np.argmax(predict)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义诗歌生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poetry(words, to_num):\n",
    "…\n",
    "#首先获取诗歌开头的标志‘B’\n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    "#并将‘B’的词向量放入模型，得到last_state\n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    "#输入起始诗句：\n",
    "        word = input('请输入起始字符:')\n",
    "        poem_ = ''\n",
    "        while word != 'E':\n",
    "poem_ += word\n",
    "#通过to_num函数转化为数字\n",
    "x = np.array(to_num(word)).reshape(1, 1)\n",
    "#把数据装入一个词典，并喂给模型，一个接一个，上一次的输出是下一次的输入：\n",
    "predict, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "#将得到的预测的词向量转化为文字\n",
    "word = to_word(predict, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是通过训练过的模型，生成古诗词的全过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们来看看如果生成带有指定风格的古诗词，其实主要过程和上面一样，唯一不一样的就是如何添加我们的指定风格的诗词风。让我们来定义带有指定风格的诗歌生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(words, to_num, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\"):\n",
    "…\n",
    "        if style_words:\n",
    "for word in style_words:\n",
    "#将指定风格的诗句，单独放入模型进行训练，原理一样。\n",
    "                x = np.array(to_num(word)).reshape(1, 1)\n",
    "                last_state = sess.run(end_points['last_state'],\n",
    "                                      feed_dict={input_data: x, end_points['initial_state']: last_state})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八、后话\n",
    "通过这个项目，可以进一步加深对于lstm模型的理解，由于该项目是一个简单的lstm实现的应用，在古诗词生成领域，还需要考虑更多问题，比如对仗问题，押韵问题，而本项目只是考虑每个字之间的关系，对于局部特征学习较好，而对于诗歌的整体特征学习较弱，有兴趣的同学，也可以尝试加入更多学习的特征，使得生成的古诗词更符合古诗词的音韵美，结构美。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 九、运行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗数量： 34646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/poetry-0\n",
      "## restore from the checkpoint ./model/poetry-0\n",
      "## start training...\n",
      "Epoch: 0, batch: 0, training loss: 4.108644\n",
      "Epoch: 0, batch: 1, training loss: 4.683749\n",
      "Epoch: 0, batch: 2, training loss: 3.827703\n",
      "Epoch: 0, batch: 3, training loss: 4.456865\n",
      "Epoch: 0, batch: 4, training loss: 3.847240\n",
      "Epoch: 0, batch: 5, training loss: 3.883971\n",
      "Epoch: 0, batch: 6, training loss: 5.248982\n",
      "Epoch: 0, batch: 7, training loss: 3.975278\n",
      "## Interrupt manually, try saving checkpoint for now...\n",
      "## Last epoch were saved, next time will start from epoch 0.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "batch_size = 64\n",
    "poetry_file = './poems.txt'\n",
    "\n",
    "'''\n",
    "第一步：处理古诗词文本\n",
    ":batch_size:batch大小\n",
    ":poetry_file:古诗词文本位置\n",
    "'''\n",
    "\n",
    "def poetry_process(batch_size=64,poetry_file='./poems.txt'):\n",
    "    # 处理完的古诗词放入poetrys列表中\n",
    "    poetrys = []\n",
    "    # 打开文件\n",
    "    with open(poetry_file,'r',encoding='utf-8') as f:\n",
    "        # 按行读取古诗词\n",
    "        for line in f:\n",
    "            try:\n",
    "                # 将古诗词的诗名和内容分开\n",
    "                title,content = line.strip().split(':')\n",
    "                # 去空格，实际上没用到，但为了确保文档中没有多余的空格，还是建议写一下\n",
    "                content = content.replace(' ','') \n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content:\n",
    "                    continue\n",
    "                # 选取长度在5～79之间的古诗词\n",
    "                if len(content) < 5 or len(content) > 79:\n",
    "                    continue\n",
    "                # 添加首尾标签\n",
    "                content = 'B' + content + 'E'\n",
    "                # 添加到poetrys列表中\n",
    "                poetrys.append(content)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # 依照每首诗的长度排序\n",
    "    # poetrys = sorted(poetrys, key=lambda poetry: len(poetry))\n",
    "    print('唐诗数量：',len(poetrys))\n",
    "\n",
    "    # 统计字出现次数\n",
    "    all_words = []\n",
    "    for poetry in poetrys:\n",
    "        all_words += [word for word in poetry]\n",
    "    counter = Counter(all_words)\n",
    "    # print(counter.items())\n",
    "    # item会把字典中的每一项变成一个2元素元组，字典变成大list\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: -x[1])\n",
    "    # 利用zip提取，因为是原生数据结构，在切片上远不如numpy的结构灵活\n",
    "    words,_ = zip(*count_pairs)\n",
    "    # print(words)\n",
    "\n",
    "    words = words[:len(words)] + (' ',)  # 后面要用' '来补齐诗句长度\n",
    "    # print(words)\n",
    "    # 字典:word->int\n",
    "    # 每个字映射为一个数字ID  \n",
    "    word_num_map = dict(zip(words,range(len(words))))\n",
    "    # 把诗词转换为向量\n",
    "    to_num = lambda word: word_num_map.get(word,len(words))\n",
    "    #print(to_num)\n",
    "    poetry_vector = [list(map(to_num,poetry)) for poetry in poetrys]\n",
    "    # print(poetry_vector)\n",
    "\n",
    "\n",
    "    # 每次取64首诗进行训练 \n",
    "    n_chunk = len(poetry_vector) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size #起始位置\n",
    "        end_index = start_index + batch_size #结束位置\n",
    "        batches = poetry_vector[start_index:end_index]\n",
    "        length = max(map(len,batches))  # 记录下最长的诗句的长度\n",
    "        xdata = np.full((batch_size,length),word_num_map[' '],np.int32)\n",
    "        for row in range(batch_size):\n",
    "            xdata[row,:len(batches[row])] = batches[row]\n",
    "        # print(len(xdata[0])) 每个batch中数据长度不相等\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:,:-1] = xdata[:,1:]\n",
    "        \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "            \"\"\"\n",
    "        x_batches.append(xdata)  # (n_chunk, batch, length)\n",
    "        y_batches.append(ydata)\n",
    "    return words,poetry_vector,to_num,x_batches,y_batches\n",
    "\n",
    "# Author : hellcat\n",
    "# Time   : 18-3-12\n",
    "\n",
    "\n",
    "\n",
    "def rnn_model(num_of_word,input_data,output_data=None,rnn_size=128,num_layers=2,batch_size=128):\n",
    "    end_points = {}\n",
    "    \"\"\"\n",
    "\n",
    "    :param num_of_word: 词的个数\n",
    "    :param input_data: 输入向量\n",
    "    :param output_data: 标签\n",
    "    :param rnn_size: 隐藏层的向量尺寸\n",
    "    :param num_layers: 隐藏层的层数\n",
    "    :param batch_size: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    '''构建RNN核心'''\n",
    "    # cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    # cell_fun = tf.contrib.rnn.GRUCell\n",
    "    '''\n",
    "BasicLSTMCell类是最基本的LSTM循环神经网络单元。 \n",
    "\n",
    "num_units: LSTM cell层中的单元数 \n",
    "forget_bias: forget gates中的偏置 \n",
    "state_is_tuple: 还是设置为True吧, 返回 (c_state , m_state)的二元组 \n",
    "activation: 状态之间转移的激活函数 \n",
    "reuse: Python布尔值, 描述是否重用现有作用域中的变量\n",
    "state_size属性：如果state_is_tuple为true的话，返回的是二元状态元祖。\n",
    "output_size属性：返回LSTM中的num_units, 也就是LSTM Cell中的单元数，在初始化是输入的num_units参数\n",
    "_call_()将类实例转化为一个可调用的对象，传入输入input和状态state，根据LSTM的计算公式, 返回new_h, 和新的状态new\n",
    "    '''\n",
    "    cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "    cell = cell_fun(rnn_size,state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers,state_is_tuple=True)\n",
    "\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1,tf.float32)\n",
    "\n",
    "    # 词向量嵌入\n",
    "    embedding = tf.get_variable('embedding',initializer=tf.random_uniform([num_of_word + 1,rnn_size],-1.0,1.0))\n",
    "    inputs = tf.nn.embedding_lookup(embedding,input_data)\n",
    "        \n",
    "    \n",
    "    outputs,last_state = tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state)\n",
    "    output = tf.reshape(outputs,[-1,rnn_size])\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size,num_of_word + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[num_of_word + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output,weights),bias=bias)\n",
    "\n",
    "    if output_data is not None:\n",
    "        labels = tf.one_hot(tf.reshape(output_data,[-1]),depth=num_of_word + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n",
    "        tf.summary.scalar('loss',total_loss)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else:\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points\n",
    "\n",
    "def to_word(predict, vocabs):\n",
    "    t = np.cumsum(predict)\n",
    "    s = np.sum(predict)\n",
    "    # sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]  # [np.argmax(predict)]\n",
    " \n",
    "\n",
    "def gen_poetry(words, to_num):\n",
    "    batch_size = 1\n",
    "    print('模型保存目录为： {}'.format('./model'))\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    " \n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        word = input('请输入起始字符:')\n",
    "        poem_ = ''\n",
    "        while word != 'E':\n",
    "            poem_ += word\n",
    "            x = np.array(to_num(word)).reshape(1, 1)\n",
    "            predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                           feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            word = to_word(predict, words)\n",
    "        print(poem_)\n",
    "        return poem_\n",
    " \n",
    " \n",
    "def generate(words, to_num, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\"):\n",
    " \n",
    "    batch_size = 1\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        if style_words:\n",
    "            for word in style_words:\n",
    "                x = np.array(to_num(word)).reshape(1, 1)\n",
    "                last_state = sess.run(end_points['last_state'],\n",
    "                                      feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    " \n",
    "        # start_words = list(\"少小离家老大回\")\n",
    "        start_words = list(input(\"请输入起始语句：\"))\n",
    "        start_word_len = len(start_words)\n",
    " \n",
    "        result = start_words.copy()\n",
    "        max_len = 200\n",
    "        for i in range(max_len):\n",
    " \n",
    "            if i < start_word_len:\n",
    "                w = start_words[i]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            else:\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "                w = to_word(predict, words)\n",
    "                # w = words[np.argmax(predict)]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                if w == 'E':\n",
    "                    break\n",
    "                result.append(w)\n",
    " \n",
    "        print(''.join(result))\n",
    "\n",
    "\n",
    "def train(words,poetry_vector,x_batches,y_batches):\n",
    "    input_data = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    output_targets = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    end_points = rnn_model(len(words),input_data=input_data,output_data=output_targets,batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    merge = tf.summary.merge_all()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        writer = tf.summary.FileWriter('./logs',sess.graph)\n",
    "        sess.run(init_op)\n",
    "\n",
    "        start_epoch = 0\n",
    "        model_dir = \"./model\"\n",
    "        epochs = 1\n",
    "        checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess,checkpoint)\n",
    "            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n",
    "            start_epoch += int(checkpoint.split('-')[-1])\n",
    "            print('## start training...')\n",
    "        try:\n",
    "            for epoch in range(start_epoch,epochs):\n",
    "                n_chunk = len(poetry_vector) // batch_size\n",
    "                for n in range(n_chunk):\n",
    "                    loss,_,_ = sess.run([\n",
    "                        end_points['total_loss'],\n",
    "                        end_points['last_state'],\n",
    "                        end_points['train_op'],\n",
    "                    ],feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                    print('Epoch: %d, batch: %d, training loss: %.6f' % (epoch,n,loss))\n",
    "                    if epoch % 5 == 0:\n",
    "                        saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "                        result = sess.run(merge,feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                        writer.add_summary(result,epoch * n_chunk + n)\n",
    "        except KeyboardInterrupt:\n",
    "            print('## Interrupt manually, try saving checkpoint for now...')\n",
    "            saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "            print('## Last epoch were saved, next time will start from epoch {}.'.format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      words,poetry_vector,to_num,x_batches,y_batches = poetry_process()\n",
    "      train(words, poetry_vector, x_batches, y_batches)\n",
    "    # gen_poetry(words, to_num)\n",
    "    # generate(words, to_num, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们演示了，可以保存断点的训练过程，模型可以根据上次结束训练的时间点开始继续训练，由于时间有限，我们就不展示完全的训练的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐诗数量： 34646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/poetry-0\n",
      "请输入起始语句：少小离家老大回\n",
      "少小离家老大回，阔麾熟似朝联。阙催条，彤列岩讼韶。以蛱楼裳，即卷。若禔瘁，坐兰气入松。裾主秋鹄，斯去传。至方节，翘宵鳞。者不还自，市池万第。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "batch_size = 64\n",
    "poetry_file = './poems.txt'\n",
    "\n",
    "'''\n",
    "第一步：处理古诗词文本\n",
    ":batch_size:batch大小\n",
    ":poetry_file:古诗词文本位置\n",
    "'''\n",
    "\n",
    "def poetry_process(batch_size=64,poetry_file='./poems.txt'):\n",
    "    # 处理完的古诗词放入poetrys列表中\n",
    "    poetrys = []\n",
    "    # 打开文件\n",
    "    with open(poetry_file,'r',encoding='utf-8') as f:\n",
    "        # 按行读取古诗词\n",
    "        for line in f:\n",
    "            try:\n",
    "                # 将古诗词的诗名和内容分开\n",
    "                title,content = line.strip().split(':')\n",
    "                # 去空格，实际上没用到，但为了确保文档中没有多余的空格，还是建议写一下\n",
    "                content = content.replace(' ','') \n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content:\n",
    "                    continue\n",
    "                # 选取长度在5～79之间的古诗词\n",
    "                if len(content) < 5 or len(content) > 79:\n",
    "                    continue\n",
    "                # 添加首尾标签\n",
    "                content = 'B' + content + 'E'\n",
    "                # 添加到poetrys列表中\n",
    "                poetrys.append(content)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # 依照每首诗的长度排序\n",
    "    # poetrys = sorted(poetrys, key=lambda poetry: len(poetry))\n",
    "    print('唐诗数量：',len(poetrys))\n",
    "\n",
    "    # 统计字出现次数\n",
    "    all_words = []\n",
    "    for poetry in poetrys:\n",
    "        all_words += [word for word in poetry]\n",
    "    counter = Counter(all_words)\n",
    "    # print(counter.items())\n",
    "    # item会把字典中的每一项变成一个2元素元组，字典变成大list\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: -x[1])\n",
    "    # 利用zip提取，因为是原生数据结构，在切片上远不如numpy的结构灵活\n",
    "    words,_ = zip(*count_pairs)\n",
    "    # print(words)\n",
    "\n",
    "    words = words[:len(words)] + (' ',)  # 后面要用' '来补齐诗句长度\n",
    "    # print(words)\n",
    "    # 字典:word->int\n",
    "    # 每个字映射为一个数字ID  \n",
    "    word_num_map = dict(zip(words,range(len(words))))\n",
    "    # 把诗词转换为向量\n",
    "    to_num = lambda word: word_num_map.get(word,len(words))\n",
    "    #print(to_num)\n",
    "    poetry_vector = [list(map(to_num,poetry)) for poetry in poetrys]\n",
    "    # print(poetry_vector)\n",
    "\n",
    "\n",
    "    # 每次取64首诗进行训练 \n",
    "    n_chunk = len(poetry_vector) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size #起始位置\n",
    "        end_index = start_index + batch_size #结束位置\n",
    "        batches = poetry_vector[start_index:end_index]\n",
    "        length = max(map(len,batches))  # 记录下最长的诗句的长度\n",
    "        xdata = np.full((batch_size,length),word_num_map[' '],np.int32)\n",
    "        for row in range(batch_size):\n",
    "            xdata[row,:len(batches[row])] = batches[row]\n",
    "        # print(len(xdata[0])) 每个batch中数据长度不相等\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:,:-1] = xdata[:,1:]\n",
    "        \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "            \"\"\"\n",
    "        x_batches.append(xdata)  # (n_chunk, batch, length)\n",
    "        y_batches.append(ydata)\n",
    "    return words,poetry_vector,to_num,x_batches,y_batches\n",
    "\n",
    "# Author : hellcat\n",
    "# Time   : 18-3-12\n",
    "\n",
    "\n",
    "\n",
    "def rnn_model(num_of_word,input_data,output_data=None,rnn_size=128,num_layers=2,batch_size=128):\n",
    "    end_points = {}\n",
    "    \"\"\"\n",
    "\n",
    "    :param num_of_word: 词的个数\n",
    "    :param input_data: 输入向量\n",
    "    :param output_data: 标签\n",
    "    :param rnn_size: 隐藏层的向量尺寸\n",
    "    :param num_layers: 隐藏层的层数\n",
    "    :param batch_size: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    '''构建RNN核心'''\n",
    "    # cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    # cell_fun = tf.contrib.rnn.GRUCell\n",
    "    '''\n",
    "BasicLSTMCell类是最基本的LSTM循环神经网络单元。 \n",
    "\n",
    "num_units: LSTM cell层中的单元数 \n",
    "forget_bias: forget gates中的偏置 \n",
    "state_is_tuple: 还是设置为True吧, 返回 (c_state , m_state)的二元组 \n",
    "activation: 状态之间转移的激活函数 \n",
    "reuse: Python布尔值, 描述是否重用现有作用域中的变量\n",
    "state_size属性：如果state_is_tuple为true的话，返回的是二元状态元祖。\n",
    "output_size属性：返回LSTM中的num_units, 也就是LSTM Cell中的单元数，在初始化是输入的num_units参数\n",
    "_call_()将类实例转化为一个可调用的对象，传入输入input和状态state，根据LSTM的计算公式, 返回new_h, 和新的状态new\n",
    "    '''\n",
    "    cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "    cell = cell_fun(rnn_size,state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers,state_is_tuple=True)\n",
    "\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1,tf.float32)\n",
    "\n",
    "    # 词向量嵌入\n",
    "    embedding = tf.get_variable('embedding',initializer=tf.random_uniform([num_of_word + 1,rnn_size],-1.0,1.0))\n",
    "    inputs = tf.nn.embedding_lookup(embedding,input_data)\n",
    "        \n",
    "    \n",
    "    outputs,last_state = tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state)\n",
    "    output = tf.reshape(outputs,[-1,rnn_size])\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size,num_of_word + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[num_of_word + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output,weights),bias=bias)\n",
    "\n",
    "    if output_data is not None:\n",
    "        labels = tf.one_hot(tf.reshape(output_data,[-1]),depth=num_of_word + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n",
    "        tf.summary.scalar('loss',total_loss)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else:\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points\n",
    "\n",
    "def to_word(predict, vocabs):\n",
    "    t = np.cumsum(predict)\n",
    "    s = np.sum(predict)\n",
    "    sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    #sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]  # [np.argmax(predict)]\n",
    " \n",
    " \n",
    "def gen_poetry(words, to_num):\n",
    "    batch_size = 1\n",
    "    print('模型保存目录为： {}'.format('./model'))\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    " \n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        word = input('请输入起始字符:')\n",
    "        poem_ = ''\n",
    "        while word != 'E':\n",
    "            poem_ += word\n",
    "            x = np.array(to_num(word)).reshape(1, 1)\n",
    "            predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                           feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            word = to_word(predict, words)\n",
    "        print(poem_)\n",
    "        return poem_\n",
    " \n",
    " \n",
    "def generate(words, to_num, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\"):\n",
    " \n",
    "    batch_size = 1\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        if style_words:\n",
    "            for word in style_words:\n",
    "                x = np.array(to_num(word)).reshape(1, 1)\n",
    "                last_state = sess.run(end_points['last_state'],\n",
    "                                      feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    " \n",
    "        # start_words = list(\"少小离家老大回\")\n",
    "        start_words = list(input(\"请输入起始语句：\"))\n",
    "        start_word_len = len(start_words)\n",
    " \n",
    "        result = start_words.copy()\n",
    "        max_len = 200\n",
    "        for i in range(max_len):\n",
    " \n",
    "            if i < start_word_len:\n",
    "                w = start_words[i]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            else:\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "                w = to_word(predict, words)\n",
    "                # w = words[np.argmax(predict)]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                if w == 'E':\n",
    "                    break\n",
    "                result.append(w)\n",
    " \n",
    "        print(''.join(result))\n",
    "\n",
    "\n",
    "def train(words,poetry_vector,x_batches,y_batches):\n",
    "    input_data = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    output_targets = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    end_points = rnn_model(len(words),input_data=input_data,output_data=output_targets,batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    merge = tf.summary.merge_all()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        writer = tf.summary.FileWriter('./logs',sess.graph)\n",
    "        sess.run(init_op)\n",
    "\n",
    "        start_epoch = 0\n",
    "        model_dir = \"./model\"\n",
    "        epochs = 1\n",
    "        checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess,checkpoint)\n",
    "            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n",
    "            start_epoch += int(checkpoint.split('-')[-1])\n",
    "            print('## start training...')\n",
    "        try:\n",
    "            for epoch in range(start_epoch,epochs):\n",
    "                n_chunk = len(poetry_vector) // batch_size\n",
    "                for n in range(n_chunk):\n",
    "                    loss,_,_ = sess.run([\n",
    "                        end_points['total_loss'],\n",
    "                        end_points['last_state'],\n",
    "                        end_points['train_op'],\n",
    "                    ],feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                    print('Epoch: %d, batch: %d, training loss: %.6f' % (epoch,n,loss))\n",
    "                    if epoch % 5 == 0:\n",
    "                        saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "                        result = sess.run(merge,feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                        writer.add_summary(result,epoch * n_chunk + n)\n",
    "        except KeyboardInterrupt:\n",
    "            print('## Interrupt manually, try saving checkpoint for now...')\n",
    "            saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "            print('## Last epoch were saved, next time will start from epoch {}.'.format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "      words,poetry_vector,to_num,x_batches,y_batches = poetry_process()\n",
    "    # train(words, poetry_vector, x_batches, y_batches)\n",
    "    # gen_poetry(words, to_num)\n",
    "      generate(words, to_num, style_words=\"狂沙将军战燕然，大漠孤烟黄河骑。\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于训练语料库的原因，在完成整体训练以后，诗词生成效果将变好。\n",
    "\n",
    "本地案例\n",
    "\n",
    "head:少小离家老大回 + style:山雨欲来风满楼：\n",
    "\n",
    "少小离家老大回，四壁百月弄鸦飞。\n",
    "扫香花间春风地，隔天倾似烂桃香。\n",
    "近来谁伴清明日，两株愁味在罗帏。\n",
    "仍通西疾空何处，轧轧凉吹日方明。\n",
    "\n",
    "head:少小离家老大回 + style:铁马冰河入梦来：\n",
    "\n",
    "少小离家老大回，化空千里便成丝。\n",
    "官抛十里同牛颔，莫碍风光雪片云。\n",
    "饮水远涛飞汉地，云连城户翠微低。\n",
    "一树铁门万象耸，白云三尺各关高。\n",
    "同言东甸西游子，谁道承阳要旧忧。\n",
    "\n",
    "少小离家老大回，含颦玉烛拂楼台。\n",
    "初齐去府芙蓉死，细缓行云向国天。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "208px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
